{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import ctime\n",
    "from random import randrange\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn import preprocessing\n",
    "register_matplotlib_converters()\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model._bayes import ARDRegression\n",
    "from sklearn.ensemble._weight_boosting import AdaBoostRegressor\n",
    "from sklearn.linear_model._bayes import BayesianRidge\n",
    "from sklearn.tree._classes import DecisionTreeRegressor \n",
    "from sklearn.linear_model._coordinate_descent import ElasticNetCV\n",
    "from sklearn.ensemble._forest import ExtraTreesRegressor\n",
    "from sklearn.gaussian_process._gpr import GaussianProcessRegressor\n",
    "from sklearn.linear_model._glm.glm import GeneralizedLinearRegressor\n",
    "from sklearn.ensemble._gb import GradientBoostingRegressor\n",
    "from sklearn.ensemble._hist_gradient_boosting.gradient_boosting import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model._huber import HuberRegressor\n",
    "from sklearn.isotonic import IsotonicRegression \n",
    "from sklearn.neighbors._regression import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model._least_angle import LarsCV\n",
    "from sklearn.linear_model._coordinate_descent import LassoCV\n",
    "from sklearn.linear_model._least_angle import LassoLarsCV\n",
    "from sklearn.linear_model._least_angle import LassoLarsIC\n",
    "from sklearn.linear_model._base import LinearRegression\n",
    "from sklearn.svm._classes import LinearSVR\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPRegressor\n",
    "from sklearn.svm._classes import NuSVR\n",
    "from sklearn.linear_model._omp import OrthogonalMatchingPursuitCV\n",
    "from sklearn.linear_model._passive_aggressive import PassiveAggressiveRegressor\n",
    "from sklearn.neighbors._regression import RadiusNeighborsRegressor\n",
    "from sklearn.ensemble._forest import RandomForestRegressor\n",
    "from sklearn.linear_model._ridge import RidgeCV\n",
    "from sklearn.linear_model._stochastic_gradient import SGDRegressor\n",
    "from sklearn.svm._classes import SVR\n",
    "from sklearn.linear_model._glm.glm import TweedieRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "estimators = [('ard', ARDRegression()),\n",
    "              ('ada', AdaBoostRegressor()),\n",
    "              ('brr', BayesianRidge()),\n",
    "              ('dtr', DecisionTreeRegressor()),\n",
    "              ('enc', ElasticNetCV()),\n",
    "              ('etr', ExtraTreesRegressor()),\n",
    "              ('gpr', GaussianProcessRegressor()),\n",
    "              ('glr', GeneralizedLinearRegressor()),\n",
    "              ('gbr', GradientBoostingRegressor()),\n",
    "              ('hgb', HistGradientBoostingRegressor()),\n",
    "              ('hur', HuberRegressor()),\n",
    "              ('knr', KNeighborsRegressor()),\n",
    "              ('ker', KernelRidge()),\n",
    "              ('lar', LarsCV()),\n",
    "              ('las', LassoCV()),\n",
    "              ('llc', LassoLarsCV()),\n",
    "              ('lli', LassoLarsIC()),\n",
    "              ('lir', LinearRegression()),\n",
    "              ('lsv', LinearSVR(max_iter=100000)),\n",
    "              ('mlp', MLPRegressor(max_iter=10000)),\n",
    "              ('nsv', NuSVR(max_iter=100000)),\n",
    "              ('par', PassiveAggressiveRegressor(max_iter=10000)),\n",
    "              ('omp', OrthogonalMatchingPursuitCV()),\n",
    "              ('rfr', RandomForestRegressor()),\n",
    "              ('sgd', SGDRegressor(max_iter=10000)),\n",
    "              ('svr', SVR(max_iter=100000)),\n",
    "              ('twr', TweedieRegressor(max_iter=10000))]\n",
    "\n",
    "festimators = [('etr', ExtraTreesRegressor()),\n",
    "              ('gpr', GaussianProcessRegressor()),\n",
    "              ('gbr', GradientBoostingRegressor()),\n",
    "              ('hgb', HistGradientBoostingRegressor()),\n",
    "              ('ker', KernelRidge()),\n",
    "              ('lir', LinearRegression()),\n",
    "              ('mlp', MLPRegressor(max_iter=10000)),\n",
    "              ('nsv', NuSVR(max_iter=100000)),\n",
    "              ('rfr', RandomForestRegressor()),\n",
    "              ('svr', SVR(max_iter=100000))]\n",
    "\n",
    "Y = pd.read_csv(\"Y_pca_30.csv\")\n",
    "\n",
    "j = 0\n",
    "nmodels = 10000\n",
    "\n",
    "stacked = pd.DataFrame(index=list(range(nmodels)),columns = ['base','meta','score'])\n",
    "for k in range(nmodels):\n",
    "    base_models = list()\n",
    "    meta_name,meta_model = random.choice(festimators)\n",
    "    base_name,base_model = random.choice(estimators)\n",
    "    sname = base_name + '_pca_30.csv'\n",
    "    base_models.append(('est0',base_model))\n",
    "\n",
    "    X = pd.read_csv(sname)\n",
    "    nest = randrange(3,7)\n",
    "    for i in range(nest):\n",
    "        base_name,base_model = random.choice(estimators)\n",
    "        est_string = 'est'+str(i+1)\n",
    "        base_models.append((est_string,base_model))\n",
    "        sname = base_name + '_pca_30.csv'\n",
    "        X = pd.concat([X, pd.read_csv(sname)],axis=1)\n",
    " \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = meta_model\n",
    "    model.fit(X_train,Y_train.to_numpy().reshape(-1))\n",
    "    score = model.score(X_test,Y_test.to_numpy().reshape(-1))\n",
    "\n",
    "    stacked.loc[j]['base'] = base_models\n",
    "    stacked.loc[j]['meta'] = meta_model\n",
    "    stacked.loc[j]['score'] = score\n",
    "\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "printdf = stacked.sort_values(by = 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "printdf.to_csv('metadf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frags = glob.glob(\"./train_fft/*\")\n",
    "test_frags = glob.glob(\"./test_fft/*\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "Y = pd.Series(0, index=np.arange(len(train_frags)))\n",
    "\n",
    "i = 0\n",
    "for file in train_frags:\n",
    "    start = './train_fft/'\n",
    "    end = '.csv'\n",
    "    seg_id = file[file.find(start)+len(start):file.rfind(end)]\n",
    "    t2e = int(train.loc[train['segment_id'] == int(seg_id)]['time_to_eruption'].values)\n",
    "    Y.iloc[i] = t2e\n",
    "    i = i + 1\n",
    "\n",
    "test_id = pd.Series(0, index=np.arange(len(test_frags)))    \n",
    "i = 0\n",
    "for file in test_frags:\n",
    "    start = './test_fft/'\n",
    "    end = '.csv'\n",
    "    seg_id = file[file.find(start)+len(start):file.rfind(end)]\n",
    "\n",
    "    test_id.iloc[i] = seg_id\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "timescaler = preprocessing.StandardScaler().fit(Y.to_numpy().reshape(-1,1))\n",
    "Y = pd.Series(timescaler.transform(Y.to_numpy().reshape(-1,1)).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pca = pd.read_csv(\"pca_30.csv\")\n",
    "X = all_pca[:len(train_frags)]\n",
    "X_test = all_pca[-len(test_frags):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9999 started at Wed Dec  2 20:38:59 2020\n",
      "\n",
      " 9998 started at Wed Dec  2 20:40:07 2020\n",
      "\n",
      " 9997 started at Wed Dec  2 20:41:36 2020\n",
      "\n",
      " 9996 started at Wed Dec  2 20:46:29 2020\n",
      "\n",
      " 9995 started at Wed Dec  2 20:47:07 2020\n",
      "\n",
      " 9994 started at Wed Dec  2 20:47:44 2020\n",
      "\n",
      " 9993 started at Wed Dec  2 20:52:24 2020\n",
      "\n",
      " 9992 started at Wed Dec  2 20:54:15 2020\n",
      "\n",
      " 9991 started at Wed Dec  2 21:00:25 2020\n",
      "\n",
      " 9990 started at Wed Dec  2 21:03:48 2020\n",
      "\n",
      " 9989 started at Wed Dec  2 21:04:48 2020\n",
      "\n",
      " 9988 started at Wed Dec  2 21:13:23 2020\n",
      "\n",
      " 9987 started at Wed Dec  2 21:14:07 2020\n",
      "\n",
      " 9986 started at Wed Dec  2 21:15:19 2020\n",
      "\n",
      " 9985 started at Wed Dec  2 21:16:34 2020\n",
      "\n",
      " 9984 started at Wed Dec  2 21:20:17 2020\n",
      "\n",
      " 9983 started at Wed Dec  2 21:22:09 2020\n",
      "\n",
      " 9982 started at Wed Dec  2 21:26:29 2020\n",
      "\n",
      " 9981 started at Wed Dec  2 21:27:25 2020\n",
      "\n",
      " 9980 started at Wed Dec  2 21:28:10 2020\n",
      "\n",
      " 9979 started at Wed Dec  2 21:32:38 2020\n",
      "\n",
      " 9978 started at Wed Dec  2 21:37:21 2020\n",
      "\n",
      " 9977 started at Wed Dec  2 21:42:02 2020\n",
      "\n",
      " 9976 started at Wed Dec  2 21:44:28 2020\n",
      "\n",
      " 9975 started at Wed Dec  2 21:49:33 2020\n",
      "\n",
      " 9974 started at Wed Dec  2 21:52:04 2020\n",
      "\n",
      " 9973 started at Wed Dec  2 21:56:08 2020\n",
      "\n",
      " 9972 started at Wed Dec  2 22:00:35 2020\n",
      "\n",
      " 9971 started at Wed Dec  2 22:01:46 2020\n",
      "\n",
      " 9970 started at Wed Dec  2 22:03:45 2020\n",
      "\n",
      " 9969 started at Wed Dec  2 22:04:27 2020\n",
      "\n",
      " 9968 started at Wed Dec  2 22:05:10 2020\n",
      "\n",
      " 9967 started at Wed Dec  2 22:06:11 2020\n",
      "\n",
      " 9966 started at Wed Dec  2 22:07:17 2020\n",
      "\n",
      " 9965 started at Wed Dec  2 22:11:37 2020\n",
      "\n",
      " 9964 started at Wed Dec  2 22:13:31 2020\n",
      "\n",
      " 9963 started at Wed Dec  2 22:14:17 2020\n",
      "\n",
      " 9962 started at Wed Dec  2 22:15:00 2020\n",
      "\n",
      " 9961 started at Wed Dec  2 22:16:44 2020\n",
      "\n",
      " 9960 started at Wed Dec  2 22:18:03 2020\n",
      "\n",
      " 9959 started at Wed Dec  2 22:21:02 2020\n",
      "\n",
      " 9958 started at Wed Dec  2 22:22:18 2020\n",
      "\n",
      " 9957 started at Wed Dec  2 22:23:14 2020\n",
      "\n",
      " 9956 started at Wed Dec  2 22:24:27 2020\n",
      "\n",
      " 9955 started at Wed Dec  2 22:25:25 2020\n",
      "\n",
      " 9954 started at Wed Dec  2 22:28:29 2020\n",
      "\n",
      " 9953 started at Wed Dec  2 22:30:02 2020\n",
      "\n",
      " 9952 started at Wed Dec  2 22:33:59 2020\n",
      "\n",
      " 9951 started at Wed Dec  2 22:35:48 2020\n",
      "\n",
      " 9950 started at Wed Dec  2 22:37:52 2020\n",
      "\n",
      " 9949 started at Wed Dec  2 22:40:00 2020\n",
      "\n",
      " 9948 started at Wed Dec  2 22:42:11 2020\n",
      "\n",
      " 9947 started at Wed Dec  2 22:44:19 2020\n",
      "\n",
      " 9946 started at Wed Dec  2 22:45:52 2020\n",
      "\n",
      " 9945 started at Wed Dec  2 22:46:26 2020\n",
      "\n",
      " 9944 started at Wed Dec  2 22:52:10 2020\n",
      "\n",
      " 9943 started at Wed Dec  2 22:53:07 2020\n",
      "\n",
      " 9942 started at Wed Dec  2 22:53:56 2020\n",
      "\n",
      " 9941 started at Wed Dec  2 22:56:59 2020\n",
      "\n",
      " 9940 started at Wed Dec  2 22:57:52 2020\n",
      "\n",
      " 9939 started at Wed Dec  2 22:58:50 2020\n",
      "\n",
      " 9938 started at Wed Dec  2 22:59:55 2020\n",
      "\n",
      " 9937 started at Wed Dec  2 23:03:39 2020\n",
      "\n",
      " 9936 started at Wed Dec  2 23:05:09 2020\n",
      "\n",
      " 9935 started at Wed Dec  2 23:05:47 2020\n",
      "\n",
      " 9934 started at Wed Dec  2 23:08:25 2020\n",
      "\n",
      " 9933 started at Wed Dec  2 23:11:21 2020\n",
      "\n",
      " 9932 started at Wed Dec  2 23:13:12 2020\n",
      "\n",
      " 9931 started at Wed Dec  2 23:16:50 2020\n",
      "\n",
      " 9930 started at Wed Dec  2 23:19:48 2020\n",
      "\n",
      " 9929 started at Wed Dec  2 23:23:06 2020\n",
      "\n",
      " 9928 started at Wed Dec  2 23:25:42 2020\n",
      "\n",
      " 9927 started at Wed Dec  2 23:28:34 2020\n",
      "\n",
      " 9926 started at Wed Dec  2 23:31:24 2020\n",
      "\n",
      " 9925 started at Wed Dec  2 23:32:33 2020\n",
      "\n",
      " 9924 started at Wed Dec  2 23:33:28 2020\n",
      "\n",
      " 9923 started at Wed Dec  2 23:35:56 2020\n",
      "\n",
      " 9922 started at Wed Dec  2 23:37:30 2020\n",
      "\n",
      " 9921 started at Wed Dec  2 23:40:37 2020\n",
      "\n",
      " 9920 started at Wed Dec  2 23:42:21 2020\n",
      "\n",
      " 9919 started at Wed Dec  2 23:45:26 2020\n",
      "\n",
      " 9918 started at Wed Dec  2 23:48:23 2020\n",
      "\n",
      " 9917 started at Wed Dec  2 23:49:21 2020\n",
      "\n",
      " 9916 started at Wed Dec  2 23:50:05 2020\n",
      "\n",
      " 9915 started at Wed Dec  2 23:55:39 2020\n",
      "\n",
      " 9914 started at Thu Dec  3 00:01:45 2020\n",
      "\n",
      " 9913 started at Thu Dec  3 00:02:48 2020\n",
      "\n",
      " 9912 started at Thu Dec  3 00:03:21 2020\n",
      "\n",
      " 9911 started at Thu Dec  3 00:05:10 2020\n",
      "\n",
      " 9910 started at Thu Dec  3 00:07:48 2020\n",
      "\n",
      " 9909 started at Thu Dec  3 00:12:18 2020\n",
      "\n",
      " 9908 started at Thu Dec  3 00:16:40 2020\n",
      "\n",
      " 9907 started at Thu Dec  3 00:24:08 2020\n",
      "\n",
      " 9906 started at Thu Dec  3 00:30:23 2020\n",
      "\n",
      " 9905 started at Thu Dec  3 00:31:04 2020\n",
      "\n",
      " 9904 started at Thu Dec  3 00:37:10 2020\n",
      "\n",
      " 9903 started at Thu Dec  3 00:37:57 2020\n",
      "\n",
      " 9902 started at Thu Dec  3 00:40:09 2020\n",
      "\n",
      " 9901 started at Thu Dec  3 00:40:44 2020\n",
      "\n",
      " 9900 started at Thu Dec  3 00:42:27 2020\n",
      "\n",
      " 9899 started at Thu Dec  3 00:43:37 2020\n",
      "\n",
      " 9898 started at Thu Dec  3 00:44:18 2020\n",
      "\n",
      " 9897 started at Thu Dec  3 00:45:13 2020\n",
      "\n",
      " 9896 started at Thu Dec  3 00:46:00 2020\n",
      "\n",
      " 9895 started at Thu Dec  3 00:49:40 2020\n",
      "\n",
      " 9894 started at Thu Dec  3 00:50:40 2020\n",
      "\n",
      " 9893 started at Thu Dec  3 00:51:31 2020\n",
      "\n",
      " 9892 started at Thu Dec  3 00:55:30 2020\n",
      "\n",
      " 9891 started at Thu Dec  3 00:56:25 2020\n",
      "\n",
      " 9890 started at Thu Dec  3 00:56:51 2020\n",
      "\n",
      " 9889 started at Thu Dec  3 01:00:23 2020\n",
      "\n",
      " 9888 started at Thu Dec  3 01:02:15 2020\n",
      "\n",
      " 9887 started at Thu Dec  3 01:02:51 2020\n",
      "\n",
      " 9886 started at Thu Dec  3 01:03:39 2020\n",
      "\n",
      " 9885 started at Thu Dec  3 01:05:29 2020\n",
      "\n",
      " 9884 started at Thu Dec  3 01:07:00 2020\n",
      "\n",
      " 9883 started at Thu Dec  3 01:07:39 2020\n",
      "\n",
      " 9882 started at Thu Dec  3 01:08:20 2020\n",
      "\n",
      " 9881 started at Thu Dec  3 01:08:52 2020\n",
      "\n",
      " 9880 started at Thu Dec  3 01:10:50 2020\n",
      "\n",
      " 9879 started at Thu Dec  3 01:11:22 2020\n",
      "\n",
      " 9878 started at Thu Dec  3 01:13:28 2020\n",
      "\n",
      " 9877 started at Thu Dec  3 01:14:36 2020\n",
      "\n",
      " 9876 started at Thu Dec  3 01:15:10 2020\n",
      "\n",
      " 9875 started at Thu Dec  3 01:16:29 2020\n",
      "\n",
      " 9874 started at Thu Dec  3 01:20:03 2020\n",
      "\n",
      " 9873 started at Thu Dec  3 01:22:20 2020\n",
      "\n",
      " 9872 started at Thu Dec  3 01:24:22 2020\n",
      "\n",
      " 9871 started at Thu Dec  3 01:26:48 2020\n",
      "\n",
      " 9870 started at Thu Dec  3 01:30:31 2020\n",
      "\n",
      " 9869 started at Thu Dec  3 01:31:10 2020\n",
      "\n",
      " 9868 started at Thu Dec  3 01:32:31 2020\n",
      "\n",
      " 9867 started at Thu Dec  3 01:34:06 2020\n",
      "\n",
      " 9866 started at Thu Dec  3 01:36:05 2020\n",
      "\n",
      " 9865 started at Thu Dec  3 01:36:54 2020\n",
      "\n",
      " 9864 started at Thu Dec  3 01:37:40 2020\n",
      "\n",
      " 9863 started at Thu Dec  3 01:39:34 2020\n",
      "\n",
      " 9862 started at Thu Dec  3 01:42:05 2020\n",
      "\n",
      " 9861 started at Thu Dec  3 01:42:49 2020\n",
      "\n",
      " 9860 started at Thu Dec  3 01:43:53 2020\n",
      "\n",
      " 9859 started at Thu Dec  3 01:44:32 2020\n",
      "\n",
      " 9858 started at Thu Dec  3 01:46:22 2020\n",
      "\n",
      " 9857 started at Thu Dec  3 01:49:24 2020\n",
      "\n",
      " 9856 started at Thu Dec  3 01:50:17 2020\n",
      "\n",
      " 9855 started at Thu Dec  3 01:51:09 2020\n",
      "\n",
      " 9854 started at Thu Dec  3 01:52:14 2020\n",
      "\n",
      " 9853 started at Thu Dec  3 01:53:20 2020\n",
      "\n",
      " 9852 started at Thu Dec  3 01:58:23 2020\n",
      "\n",
      " 9851 started at Thu Dec  3 02:00:22 2020\n",
      "\n",
      " 9850 started at Thu Dec  3 02:00:57 2020\n",
      "\n",
      " 9849 started at Thu Dec  3 02:01:59 2020\n",
      "\n",
      " 9848 started at Thu Dec  3 02:03:57 2020\n",
      "\n",
      " 9847 started at Thu Dec  3 02:04:46 2020\n",
      "\n",
      " 9846 started at Thu Dec  3 02:09:16 2020\n",
      "\n",
      " 9845 started at Thu Dec  3 02:13:14 2020\n",
      "\n",
      " 9844 started at Thu Dec  3 02:17:11 2020\n",
      "\n",
      " 9843 started at Thu Dec  3 02:18:45 2020\n",
      "\n",
      " 9842 started at Thu Dec  3 02:19:53 2020\n",
      "\n",
      " 9841 started at Thu Dec  3 02:24:00 2020\n",
      "\n",
      " 9840 started at Thu Dec  3 02:28:43 2020\n",
      "\n",
      " 9839 started at Thu Dec  3 02:30:29 2020\n",
      "\n",
      " 9838 started at Thu Dec  3 02:34:48 2020\n",
      "\n",
      " 9837 started at Thu Dec  3 02:37:34 2020\n",
      "\n",
      " 9836 started at Thu Dec  3 02:38:40 2020\n",
      "\n",
      " 9835 started at Thu Dec  3 02:39:21 2020\n",
      "\n",
      " 9834 started at Thu Dec  3 02:40:37 2020\n",
      "\n",
      " 9833 started at Thu Dec  3 02:45:05 2020\n",
      "\n",
      " 9832 started at Thu Dec  3 02:47:10 2020\n",
      "\n",
      " 9831 started at Thu Dec  3 02:47:43 2020\n",
      "\n",
      " 9830 started at Thu Dec  3 02:49:50 2020\n",
      "\n",
      " 9829 started at Thu Dec  3 02:50:38 2020\n",
      "\n",
      " 9828 started at Thu Dec  3 02:55:11 2020\n",
      "\n",
      " 9827 started at Thu Dec  3 02:56:45 2020\n",
      "\n",
      " 9826 started at Thu Dec  3 02:57:28 2020\n",
      "\n",
      " 9825 started at Thu Dec  3 02:58:52 2020\n",
      "\n",
      " 9824 started at Thu Dec  3 03:00:23 2020\n",
      "\n",
      " 9823 started at Thu Dec  3 03:02:02 2020\n",
      "\n",
      " 9822 started at Thu Dec  3 03:05:50 2020\n",
      "\n",
      " 9821 started at Thu Dec  3 03:08:22 2020\n",
      "\n",
      " 9820 started at Thu Dec  3 03:09:01 2020\n",
      "\n",
      " 9819 started at Thu Dec  3 03:09:53 2020\n",
      "\n",
      " 9818 started at Thu Dec  3 03:14:00 2020\n",
      "\n",
      " 9817 started at Thu Dec  3 03:15:56 2020\n",
      "\n",
      " 9816 started at Thu Dec  3 03:22:31 2020\n",
      "\n",
      " 9815 started at Thu Dec  3 03:23:36 2020\n",
      "\n",
      " 9814 started at Thu Dec  3 03:24:20 2020\n",
      "\n",
      " 9813 started at Thu Dec  3 03:24:57 2020\n",
      "\n",
      " 9812 started at Thu Dec  3 03:28:11 2020\n",
      "\n",
      " 9811 started at Thu Dec  3 03:29:08 2020\n",
      "\n",
      " 9810 started at Thu Dec  3 03:31:28 2020\n",
      "\n",
      " 9809 started at Thu Dec  3 03:35:35 2020\n",
      "\n",
      " 9808 started at Thu Dec  3 03:39:24 2020\n",
      "\n",
      " 9807 started at Thu Dec  3 03:40:03 2020\n",
      "\n",
      " 9806 started at Thu Dec  3 03:42:48 2020\n",
      "\n",
      " 9805 started at Thu Dec  3 03:43:28 2020\n",
      "\n",
      " 9804 started at Thu Dec  3 03:45:44 2020\n",
      "\n",
      " 9803 started at Thu Dec  3 03:46:24 2020\n",
      "\n",
      " 9802 started at Thu Dec  3 03:47:42 2020\n",
      "\n",
      " 9801 started at Thu Dec  3 03:49:38 2020\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model._bayes import ARDRegression\n",
    "from sklearn.ensemble._weight_boosting import AdaBoostRegressor\n",
    "from sklearn.linear_model._bayes import BayesianRidge\n",
    "from sklearn.tree._classes import DecisionTreeRegressor \n",
    "from sklearn.linear_model._coordinate_descent import ElasticNetCV\n",
    "from sklearn.ensemble._forest import ExtraTreesRegressor\n",
    "from sklearn.gaussian_process._gpr import GaussianProcessRegressor\n",
    "from sklearn.linear_model._glm.glm import GeneralizedLinearRegressor\n",
    "from sklearn.ensemble._gb import GradientBoostingRegressor\n",
    "from sklearn.ensemble._hist_gradient_boosting.gradient_boosting import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model._huber import HuberRegressor\n",
    "from sklearn.isotonic import IsotonicRegression \n",
    "from sklearn.neighbors._regression import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model._least_angle import LarsCV\n",
    "from sklearn.linear_model._coordinate_descent import LassoCV\n",
    "from sklearn.linear_model._least_angle import LassoLarsCV\n",
    "from sklearn.linear_model._least_angle import LassoLarsIC\n",
    "from sklearn.linear_model._base import LinearRegression\n",
    "from sklearn.svm._classes import LinearSVR\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPRegressor\n",
    "from sklearn.svm._classes import NuSVR\n",
    "from sklearn.linear_model._omp import OrthogonalMatchingPursuitCV\n",
    "from sklearn.linear_model._passive_aggressive import PassiveAggressiveRegressor\n",
    "from sklearn.neighbors._regression import RadiusNeighborsRegressor\n",
    "from sklearn.ensemble._forest import RandomForestRegressor\n",
    "from sklearn.linear_model._ridge import RidgeCV\n",
    "from sklearn.linear_model._stochastic_gradient import SGDRegressor\n",
    "from sklearn.svm._classes import SVR\n",
    "from sklearn.linear_model._glm.glm import TweedieRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_fold = 5\n",
    "cv = KFold(n_splits=n_fold, shuffle=True, random_state=420)\n",
    "\n",
    "for i in range(9999,9800,-1):   \n",
    "    r2 = 0\n",
    "    predicted_times = np.zeros(len(X_test))\n",
    "    estimators = printdf.iloc[i]['base']\n",
    "    festimator = printdf.iloc[i]['meta']\n",
    "    model = StackingRegressor(estimators=estimators,final_estimator=festimator)\n",
    "    print('\\n',i, 'started at', ctime())\n",
    "    for fold_n, (train_index, valid_index) in enumerate(cv.split(X)):\n",
    "\n",
    "\n",
    "        X_train = X.iloc[train_index,:]\n",
    "        X_valid = X.iloc[valid_index,:]\n",
    "\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_valid = Y.iloc[valid_index]\n",
    "\n",
    "        model.fit(X_train,Y_train)\n",
    "        r2 += model.score(X_valid,Y_valid)\n",
    "        predicted_times += model.predict(X_test)\n",
    "\n",
    "    r2 /= n_fold\n",
    "    predicted_times /= n_fold\n",
    "\n",
    "    predicted_times = timescaler.inverse_transform(predicted_times)\n",
    "    submission = pd.DataFrame({\n",
    "            \"segment_id\": test_id,\n",
    "            \"time_to_eruption\": predicted_times\n",
    "        })\n",
    "    file = 'submission_' + str(i) + '.csv'\n",
    "    submission.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9800 started at Thu Dec  3 03:50:43 2020\n",
      "\n",
      " 9799 started at Thu Dec  3 03:51:46 2020\n",
      "\n",
      " 9798 started at Thu Dec  3 03:52:40 2020\n",
      "\n",
      " 9797 started at Thu Dec  3 03:54:39 2020\n",
      "\n",
      " 9796 started at Thu Dec  3 03:57:47 2020\n",
      "\n",
      " 9795 started at Thu Dec  3 03:59:00 2020\n",
      "\n",
      " 9794 started at Thu Dec  3 04:00:39 2020\n",
      "\n",
      " 9793 started at Thu Dec  3 04:04:49 2020\n",
      "\n",
      " 9792 started at Thu Dec  3 04:07:26 2020\n",
      "\n",
      " 9791 started at Thu Dec  3 04:08:29 2020\n",
      "\n",
      " 9790 started at Thu Dec  3 04:10:01 2020\n",
      "\n",
      " 9789 started at Thu Dec  3 04:10:35 2020\n",
      "\n",
      " 9788 started at Thu Dec  3 04:12:47 2020\n",
      "\n",
      " 9787 started at Thu Dec  3 04:13:57 2020\n",
      "\n",
      " 9786 started at Thu Dec  3 04:17:42 2020\n",
      "\n",
      " 9785 started at Thu Dec  3 04:21:22 2020\n",
      "\n",
      " 9784 started at Thu Dec  3 04:22:06 2020\n",
      "\n",
      " 9783 started at Thu Dec  3 04:24:07 2020\n",
      "\n",
      " 9782 started at Thu Dec  3 04:25:41 2020\n",
      "\n",
      " 9781 started at Thu Dec  3 04:26:40 2020\n",
      "\n",
      " 9780 started at Thu Dec  3 04:28:17 2020\n",
      "\n",
      " 9779 started at Thu Dec  3 04:29:01 2020\n",
      "\n",
      " 9778 started at Thu Dec  3 04:30:41 2020\n",
      "\n",
      " 9777 started at Thu Dec  3 04:35:07 2020\n",
      "\n",
      " 9776 started at Thu Dec  3 04:36:10 2020\n",
      "\n",
      " 9775 started at Thu Dec  3 04:39:50 2020\n",
      "\n",
      " 9774 started at Thu Dec  3 04:40:26 2020\n",
      "\n",
      " 9773 started at Thu Dec  3 04:43:08 2020\n",
      "\n",
      " 9772 started at Thu Dec  3 04:44:37 2020\n",
      "\n",
      " 9771 started at Thu Dec  3 04:46:30 2020\n",
      "\n",
      " 9770 started at Thu Dec  3 04:48:05 2020\n",
      "\n",
      " 9769 started at Thu Dec  3 04:52:16 2020\n",
      "\n",
      " 9768 started at Thu Dec  3 04:53:17 2020\n",
      "\n",
      " 9767 started at Thu Dec  3 04:58:35 2020\n",
      "\n",
      " 9766 started at Thu Dec  3 05:00:11 2020\n",
      "\n",
      " 9765 started at Thu Dec  3 05:02:11 2020\n",
      "\n",
      " 9764 started at Thu Dec  3 05:03:13 2020\n",
      "\n",
      " 9763 started at Thu Dec  3 05:06:36 2020\n",
      "\n",
      " 9762 started at Thu Dec  3 05:07:13 2020\n",
      "\n",
      " 9761 started at Thu Dec  3 05:08:38 2020\n",
      "\n",
      " 9760 started at Thu Dec  3 05:09:54 2020\n",
      "\n",
      " 9759 started at Thu Dec  3 05:11:30 2020\n",
      "\n",
      " 9758 started at Thu Dec  3 05:14:18 2020\n",
      "\n",
      " 9757 started at Thu Dec  3 05:18:00 2020\n",
      "\n",
      " 9756 started at Thu Dec  3 05:19:36 2020\n",
      "\n",
      " 9755 started at Thu Dec  3 05:22:09 2020\n",
      "\n",
      " 9754 started at Thu Dec  3 05:23:10 2020\n",
      "\n",
      " 9753 started at Thu Dec  3 05:23:59 2020\n",
      "\n",
      " 9752 started at Thu Dec  3 05:25:50 2020\n",
      "\n",
      " 9751 started at Thu Dec  3 05:28:05 2020\n",
      "\n",
      " 9750 started at Thu Dec  3 05:30:30 2020\n",
      "\n",
      " 9749 started at Thu Dec  3 05:31:13 2020\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "cv = KFold(n_splits=n_fold, shuffle=True, random_state=420)\n",
    "\n",
    "for i in range(9800,9000,-1):   \n",
    "    r2 = 0\n",
    "    predicted_times = np.zeros(len(X_test))\n",
    "    estimators = printdf.iloc[i]['base']\n",
    "    festimator = printdf.iloc[i]['meta']\n",
    "    model = StackingRegressor(estimators=estimators,final_estimator=festimator)\n",
    "    print('\\n',i, 'started at', ctime())\n",
    "    for fold_n, (train_index, valid_index) in enumerate(cv.split(X)):\n",
    "\n",
    "\n",
    "        X_train = X.iloc[train_index,:]\n",
    "        X_valid = X.iloc[valid_index,:]\n",
    "\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_valid = Y.iloc[valid_index]\n",
    "\n",
    "        model.fit(X_train,Y_train)\n",
    "        r2 += model.score(X_valid,Y_valid)\n",
    "        predicted_times += model.predict(X_test)\n",
    "\n",
    "    r2 /= n_fold\n",
    "    predicted_times /= n_fold\n",
    "\n",
    "    predicted_times = timescaler.inverse_transform(predicted_times)\n",
    "    submission = pd.DataFrame({\n",
    "            \"segment_id\": test_id,\n",
    "            \"time_to_eruption\": predicted_times\n",
    "        })\n",
    "    file = 'submission_' + str(i) + '.csv'\n",
    "    submission.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
